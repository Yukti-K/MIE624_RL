{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yukti-K/MIE624_RL/blob/main/10_QLearning/10a_QL_FrozenLake_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UHsf5Wq8ehBV"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "#Lapan text book Chapter 6, example 1\n",
        "import gym\n",
        "import collections\n",
        "#from gym.utils.play import play\n",
        "#from tensorboardX import SummaryWriter\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "TEST_EPISODES = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "'''Things to try\n",
        "1. Epsilon greedy action (set exploration_prob=0.1) v. random action selection policy (set exploration_prob = 1)\n",
        "2. Learning rate\n",
        "'''\n"
      ],
      "metadata": {
        "id": "AXFBWSxHfRDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning:\n",
        "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=1, exploration_prob=1):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_prob = exploration_prob\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "\n",
        "    def choose_action(self, state, random_action):\n",
        "        if random_action:\n",
        "            return np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "\n",
        "    def update_q_table(self, state, action, next_state, reward):\n",
        "        self.q_table =\n",
        "\n",
        "    def update_rates(self, iteration):\n",
        "        self.learning_rate = max(0.1, min(1.0, 1.0 - math.log10((iteration + 1) / 25)))\n",
        "\n",
        "    def play_episode(self, env): #play learned policy and return total reward\n",
        "        state = env.reset()\n",
        "        done = 10\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action = np.argmax(self.q_table[state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            done -= 1\n",
        " #         if total_reward > 0.80:\n",
        " #             done = True\n",
        " #             print(\"Solved in %d iterations!\" % episode)\n",
        " #             break\n",
        "        return total_reward\n",
        "\n",
        "    def get_q_table(self):\n",
        "        return self.q_table\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iLmTJmZcfAmx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(ENV_NAME)\n",
        "    #env =gym.make(ENV_NAME, render_mode=\"human\")\n",
        "    ql_agent = QLearning(env.observation_space.n, env.action_space.n)\n",
        "    #writer = SummaryWriter(comment=\"QL_frozen lake\")\n",
        "    NUM_EPISODES = 10000\n",
        "    best_reward = 0.0\n",
        "    for episode in range(NUM_EPISODES):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "          action = ql_agent.choose_action(state, env.action_space.sample())#Choose an action\n",
        "          next_state, reward, done, _ = env.step(action) # Take the chosen action and observe the next state and reward\n",
        "          ql_agent.update_q_table(state, action, next_state, reward) #update Q-factors\n",
        "\n",
        "          state = next_state\n",
        "\n",
        "        # Evaluate the learned Q-table\n",
        "        total_reward = 0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            total_reward += ql_agent.play_episode(env)\n",
        "\n",
        "        average_reward = total_reward / TEST_EPISODES\n",
        "        #writer.add_scalar(\"reward\", average_reward, episode)\n",
        "        if average_reward > best_reward:\n",
        "            print(\"Best reward updated %.3f -> %.3f\" % (\n",
        "                best_reward, average_reward))\n",
        "            best_reward = average_reward\n",
        "        if average_reward > 0.80:\n",
        "            print(\"Solved in %d iterations!\" % episode)\n",
        "            break\n",
        "\n",
        "    # Evaluate the learned Q-table\n",
        "    #env = gym.wrappers.Monitor(env, \"recording\")\n",
        "    q_table=ql_agent.get_q_table()\n",
        "    print(q_table)\n",
        "    #writer.close()\n",
        "#tensorboard --logdir runs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mDm-K4GfbKa",
        "outputId": "2f0ecd31-0ece-499d-9ee7-7c401858b3bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tdwPCmdsTVbM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}